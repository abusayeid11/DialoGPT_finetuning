Question,Answer
What is ensemble learning?,"Ensemble learning is an ML technique that combines predictions from multiple models (called ""weak learners"") to produce a stronger, more accurate model."
What are the common conditions for ensemble modeling?,"The main conditions are:

Using different model algorithms (e.g., Stacking).

Changing model parameters (e.g., Stacking).

Modifying the training dataset (e.g., Bagging, Boosting).

Altering the feature set (e.g., Random Forest)."
 How does ensemble learning improve model performance?," It helps by reducing overfitting, improving generalization, and creating more robust models compared to single learners."
 What is a major limitation of ensemble learning?," It increases computational complexity and training time since multiple models must be trained and combined, making it slower for real-time applications."
Which ensemble method uses different subsets of features?,Random Forest uses random subsets of features to build diverse decision trees.
How does Bagging differ from Boosting?,"Bagging trains models in parallel with random data subsets, while Boosting trains sequentially, focusing on correcting errors."
What is the main idea behind ensemble learning?,Ensemble learning combines predictions from multiple models (weak learners) to produce a more accurate and robust final prediction than any single model alone.
What are the four main conditions for effective ensemble modeling?,"Different model algorithms (e.g., combining Decision Trees, SVM, and Logistic Regression in Stacking).Varying model parameters (e.g., tuning hyperparameters in the same algorithm).Changing the training dataset (e.g., Bagging and Boosting use resampling techniques).Modifying the attribute/feature set (e.g., Random Forest uses random feature subsets)"
How does changing the training dataset help in ensemble learning?,"Techniques like Bagging (Bootstrap Aggregating) and Boosting create multiple training subsets (with replacement or weighted samples), reducing variance and bias."
Why does Random Forest use a different subset of features for each tree?,"Random Forest randomly selects features for each decision tree, ensuring diversity among models and reducing overfitting."
What is the key benefit of ensemble learning in reducing overfitting?,"By averaging or voting across multiple models, ensemble methods balance errors, leading to better generalization on unseen data."
How does Boosting differ from Bagging?,"Bagging trains models in parallel with random data samples (e.g., Random Forest).

Boosting trains models sequentially, where each new model corrects errors of the previous one (e.g., AdaBoost, Gradient Boosting)."
What is Stacking in ensemble learning?,Stacking combines multiple models (base learners) and uses another model (meta-learner) to make the final prediction based on their outputs.
What is a major drawback of ensemble methods?,They require more computational power and training time since multiple models must be trained and combined.
Can ensemble learning work with any type of model?,"Yes, as long as the base models are diverse (e.g., different algorithms or training sets), ensemble methods can improve performance."
Why is model diversity important in ensemble learning?," If all models make similar errors, combining them wonâ€™t improve results. Diversity ensures errors cancel out, improving overall accuracy."
Bagging example algorithms,Trains models on random data subsets (with replacement) - Random Forest
Boosting example algorithms,"Sequentially corrects errors by weighting misclassified samples- AdaBoost, XGBoost"
Stacking example algorithms,"Combines multiple models via a meta-learner-Custom MLP/Logistic Regression as meta-learner
"