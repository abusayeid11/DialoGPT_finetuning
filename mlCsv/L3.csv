"question","answer"
"What are the main types of supervised machine learning problems?","There are two major types of supervised machine learning problems: Classification – used to predict discrete labels or categories. Regression – used to predict continuous numerical values."
"What is classification in machine learning?","Classification refers to predicting a label or category for input data. It includes two subtypes: Binary classification: distinguishing between exactly two classes. Multiclass classification: distinguishing between more than two classes."
"What is binary classification?","Binary classification answers a yes/no question. For example, classifying emails as spam or not spam is a binary classification problem."
"What is multiclass classification?","Multiclass classification predicts one class from more than two possible categories. An example is predicting a website’s language based on its text content."
"What is regression in machine learning?","Regression tasks involve predicting a continuous numeric value. Examples include predicting a person’s income based on attributes like education, age, and location, or predicting crop yield based on factors like past yields and weather."
"How do you distinguish classification from regression?","If the output has continuity (can take on a wide range of numeric values), it's a regression problem. If the output is categorical, it's a classification problem."
"What does generalization mean in supervised learning?","Generalization is the ability of a trained model to make accurate predictions on new, unseen data that has similar characteristics to the training data."
"What is overfitting in machine learning?","Overfitting happens when a model is too complex and fits the training data too closely, capturing noise instead of the underlying pattern. Such a model performs poorly on unseen data."
"What is underfitting in machine learning?","Underfitting occurs when a model is too simple to capture the underlying structure of the data, resulting in poor performance even on the training set."
"How does model complexity affect accuracy?","A more complex model usually fits the training data better, but if it becomes too complex, it may overfit and fail to generalize. The goal is to find a balance—a 'sweet spot' that provides good generalization."
"What is the ideal model in terms of complexity?","The ideal model has just enough complexity to capture the important patterns in the data without overfitting. This yields the best generalization performance."
"How is model complexity related to dataset size?","Model complexity should match the variety in your dataset. A larger, more varied dataset allows you to use more complex models. Simply duplicating data or using very similar points does not help; true diversity in data is needed to support complex models."
"What is the K-Nearest Neighbors (KNN) algorithm?","KNN is one of the simplest machine learning algorithms. It works by storing the training data and, during prediction, finding the closest data points—or 'nearest neighbors'—to make a decision."
"How does the KNN algorithm make predictions?","To predict a new data point: Calculate proximity (distance) to all training points. Select the optimal value of K (number of neighbors). Identify the K-nearest neighbors. For classification, use majority voting of the neighbors' labels. For regression, take the average of the neighbors' target values."
"What is a measure of proximity in KNN?","A measure of proximity evaluates how similar a test data point is to training data points. This can include: Distance, Weights, Correlation similarity, Simple matching coefficient, Jaccard similarity, Cosine similarity"
"Can KNN handle categorical inputs?","Yes, but for categorical attributes, the distance is typically treated as 1 (not equal) or 0 (equal). To ensure fair comparison, input values are usually normalized."
"Why is normalization important in KNN?","Because KNN is distance-based, different attribute scales can distort the results. Normalizing all attributes ensures that no single feature dominates the distance computation."
"What are common distance metrics used in KNN?","Euclidean Distance: Square root of the sum of squared differences across attributes. Manhattan Distance: Sum of the absolute differences across attributes. Chebyshev Distance: Maximum difference across any attribute."
"Why is KNN called a Lazy Learner?","KNN does not learn a model during training. Instead, it memorizes the data and performs computations only during prediction. Because it delays learning until prediction time, it is called a 'Lazy Learner.'"
"Why is the value of K important in KNN?","The choice of K (number of neighbors) affects model accuracy. Too small a K may lead to noisy predictions; too large a K may oversimplify. Optimal K can be determined using: Square Root Method: K = √(number of samples), adjust to an odd number. Cross-Validation: Try multiple K values and pick the one with the best performance. Elbow Method: Plot error rate vs. K and choose where the error stabilizes. Domain Knowledge: Sometimes expert knowledge helps decide K."
"How are distances and neighbors calculated in KNN?","For a given test point, distances to all training points are computed. These distances are ranked, and the top K points are selected. For example, in the iris dataset, the closest 5 neighbors determine the class label."
"What are the advantages of the KNN algorithm?","No Training Phase: Learns only during prediction. Fast and Simple: Quick to implement and understand. Few Parameters: Only requires the value of K and a proximity measure."
"What are the disadvantages of the KNN algorithm?", "Slow on Large Datasets: Distance calculations become costly. Poor Performance on High Dimensions: Distances become less meaningful. Needs Feature Scaling: Without normalization, results may be inaccurate. Sensitive to Noise and Outliers: Outliers and missing values can severely impact accuracy."
"What is the full for of KNN?","Kth Nearest Neighbors"
"Can KNN solve both classification and regression tasks?","KNN uses voting for Classification problems and take average for Regression tasks."
"How KNN solve classification problems?", "In the classification problem, the class labels of K-nearest neighbors are determined by performing majority voting. The class with the most occurrences among the neighbors becomes the predicted class for the target data point."
"How KNN solve regression problems?","In the regression problem, the class label is calculated by taking average of the target values of K nearest neighbors. The calculated average value becomes the predicted output for the target data point."
"How can we detemine if one data is close to another?","To quantify similarity between two records, there is a range of techniques available such as Distance, Weights, Correlation similarity, Simple matching coefficient, Jaccard similarity, Cosine similarity." 
"What is Euclidean Distance?","The Euclidean distance is the root of the sum of the squared differences of all attributes in the dataset."
"What is Manhattan distance?","The Manhattan distance is the sum of the difference between individual attributes." 
"What us Chebishev distance?","The Chebyshev distance is the maximum difference between all attributes in the dataset." 
"How can value of K can be determined?","There is no one proper method of finding the K value in the KNN algorithm. But you should try the following suggestions: k can be square root of total data, and should be odd. K can be cross validated with iteration eg: k=1,3,5,... Sometimes k can be decided from domain knowledge."
"What is the Square Root Method for determining of K in KNN?","Square Root Method: K should be the square root of the number of samples in the training dataset. K should be chosen as the odd so that there are no ties. If the square root is even, then add or subtract 1 to it." 
"What is the Cross-Validation Method for determining of K in KNN?","Cross-validation can be used to find out the optimal value of K in KNN. Start with the minimum value of k i.e, K=1 , and run cross-validation, measure the accuracy, and keep repeating till the results become consistent. As the value of K increases, the error usually goes down after each one-step increase in K, then stabilizes, and then raises again. Finally, pick the optimum K at the beginning of the stable zone. This technique is also known as the Elbow Method." 
"Do KNN require training?","No Training Period is required for KNN algorithm."
"How many parameters are required in KNN algorithm?","To implement the KNN algorithm, we need only two parameters i.e. the value of K and the measure of proximity."
"Is KNN the best model?", "It's simple and effective but does not work well with large datasets"
"Why KNN performs bad for large datasets?","In large datasets, the cost of calculating the distance between the new point and each existing point is huge which decreases the performance of the algorithm."
"Why KNN performs poor with high dimention data?","KNN Does not work well with high dimensions. With the increasing number of dimensions, it becomes difficult to calculate the distance for each dimension."
"Do we need to normalize data in KNN?","We need to do feature scaling (standardization and normalization) on the dataset before feeding it to the KNN algorithm otherwise it may generate wrong predictions."
"Can KNN model deal with outliers?","KNN is highly sensitive to the noise present in the dataset and requires manual imputation of the missing values along with outliers removal."